{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some code is shamelessly copied from Learning Blogs\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from pylab import rcParams\n",
    "import math\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import random\n",
    "from functools import reduce\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "rcParams['figure.figsize'] = 15, 5\n",
    "\n",
    "def gt(x,y): \n",
    "    if x > y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def le(x,y): \n",
    "    if x <= y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def parseDate(d):\n",
    "    try:\n",
    "        return parser.parse(d)        \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def maxD(v):\n",
    "    dt1 = parseDate(v[0])\n",
    "    dt2 = parseDate(v[1])\n",
    "    if dt1 is not None and dt2 is not None:\n",
    "        return max(dt1, dt2)\n",
    "    elif dt1 is not None:\n",
    "        return dt1\n",
    "    else:\n",
    "        return dt2\n",
    "    \n",
    "def addDelta(v):\n",
    "    delta = datetime.timedelta(0,random.randint(1, 15), random.randint(1, 1001))\n",
    "    dt = parseDate(str(v[0]))\n",
    "    if dt is not None:\n",
    "        return dt+delta\n",
    "    else:\n",
    "        return dt\n",
    "    \n",
    "\n",
    "    \n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s[0])\n",
    "    except ValueError:\n",
    "        return float(s[0])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def differ(v):\n",
    "    d1 = num(v[0])\n",
    "    d2 = num(v[1])\n",
    "    if d1 is not None and d2 is not None and abs(d1-d2)<=10:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def pererr(err, avg):\n",
    "    return (err/avg)*100\n",
    "\n",
    "def mean(l):\n",
    "    return reduce(lambda x, y: x + y, l) / len(l)\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return np.array(diff)\n",
    "\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, pred, interval=1):\n",
    "    return pred + history[-interval]\n",
    "\n",
    "    \n",
    "def predict(coef, history):\n",
    "    pred = 0.0\n",
    "    for i in range(1, len(coef)+1):\n",
    "        pred += coef[i-1] * history[-i]\n",
    "    return pred    \n",
    "\n",
    "def calculateErrs(test, pred):\n",
    "    errs = []\n",
    "    for i in range(len(test)):\n",
    "        err=pererr(abs(test[i]-pred[i]), test[i])\n",
    "        errs.append(err)\n",
    "    rmse = sqrt(mean_squared_error(test, pred))\n",
    "    mae = mean_absolute_error(test, pred)\n",
    "    print('Test RMSE: %.3f, MAE:%.3f, Err:%.3f'% (rmse, mae, mean(errs)))\n",
    "    return errs\n",
    "    \n",
    "    \n",
    "def errorRolls(errs):    \n",
    "    counters = [np.array([le(x,2), le(x,5), le(x,7), le(x,10), le(x,15), gt(x,15)]) for x in errs]\n",
    "    total = len(counters)*1.0\n",
    "    errorRollUp = zip([2, 3, 5, 7, 10, 15, 15], reduce((lambda x,y : x+y), counters))\n",
    "    return [(x[0], ((x[1]/total)*100)) for x in errorRollUp] \n",
    "\n",
    "## CLUB DataFrame Based on TruckTypes\n",
    "def clubTruckTypes(truck_types_to_club, kpi_groupBy):\n",
    "    all_groups=[]\n",
    "    tobe_clubbed={}\n",
    "    ## Group certain DF based on TruckTypes\n",
    "    for (x, y) in kpi_groupBy:\n",
    "        if x[3] in truck_types_to_club:\n",
    "            key = (x[0], x[1], x[2])\n",
    "            if tobe_clubbed.get(key):\n",
    "                tobe_clubbed[key].append(y)\n",
    "            else:\n",
    "                tobe_clubbed[key] = [y]\n",
    "        else:\n",
    "            all_groups.append((x, y))            \n",
    "    for (k,v) in tobe_clubbed.iteritems():\n",
    "        key = k+(truck_types_to_club[0],)\n",
    "        all_groups.append((key, pd.concat(v)))        \n",
    "    return all_groups\n",
    "\n",
    "## Convert DATAFrame to Series with Resampling & Forward/Backword Filling NANs\n",
    "def getSeriesWithResampleAndFNA(kpi, groupBY, truck_types_to_club, ptr=\"ptr\", date=\"d\", W=8):\n",
    "    kpi_ = kpi[(pd.isnull(kpi[ptr])==False)]\n",
    "    kpi_groupBy = kpi_.groupby(groupBY)\n",
    "    kpi_groupBy = [x for x in list(kpi_groupBy) if len(x[1]) >= W]\n",
    "    if truck_types_to_club:\n",
    "        kpi_groupBy =  clubTruckTypes(truck_types_to_club, kpi_groupBy)   \n",
    "    all_series = [(x[0], Series(x[1][ptr].values, index=x[1][date].values)) for x in kpi_groupBy]\n",
    "    all_series = [(x, s.sort_index()) for (x, s) in all_series]\n",
    "    all_series = [(x, s.resample(\"1d\").fillna(method=\"ffill\").fillna(method=\"bfill\")) for (x, s) in all_series]\n",
    "    return [(x, s) for (x, s) in all_series if len(s) > W]\n",
    "\n",
    "\n",
    "## Convert DATAFrame to Series without Any Resampling\n",
    "def getSeriesWithoutResample(kpi, groupBY, truck_types_to_club, ptr=\"ptr\", date=\"d\", W=8):\n",
    "    kpi_ = kpi[(pd.isnull(kpi[ptr])==False)]\n",
    "    kpi_groupBy = kpi_.groupby(groupBY)\n",
    "    if truck_types_to_club:\n",
    "        kpi_groupBy =  clubTruckTypes(truck_types_to_club, kpi_groupBy)   \n",
    "    kpi_groupBy = [x for x in list(kpi_groupBy) if len(x[1]) >= W]\n",
    "    all_series = [(x[0], Series(x[1][ptr].values, index=x[1][date].values)) for x in kpi_groupBy]\n",
    "    all_series = [(x, s.sort_index()) for (x, s) in all_series]\n",
    "    return [(x, s) for (x, s) in all_series if len(s) >= W]\n",
    "\n",
    "## get moving window features from the series for W size window with differencing\n",
    "def getMovingWindowFeatures(series, W, last=False):\n",
    "    X = series.values\n",
    "    # converting time to epoch\n",
    "    time = [t.to_datetime64().astype(np.int64)//10 ** 9 for t in series.index]\n",
    "    # move each window(W+2) by 1  data point\n",
    "    # keep last chunk for prediction\n",
    "    NWs = len(X)-W-1 \n",
    "\n",
    "    # Features should have dimensions (NWs x WSize)\n",
    "    # Labels should have dimensions (NWs)\n",
    "    feat = np.zeros( [NWs, 2*W] ) - 9999999.\n",
    "    labl = np.zeros( [NWs] ) - 99999\n",
    "    delta = [0]*NWs\n",
    "    \n",
    "    # Now fill the train feature and label arrays with differencing\n",
    "    for i in range(NWs):\n",
    "        feat[i,:] = np.concatenate((time[i:i+W] - time[i+W], X[i:i+W] - X[i+W]))        \n",
    "        assert np.isfinite(feat[i, :]).all(), (W, len(X), NWs, i, series)\n",
    "        labl[i]   = X[i+W+1] - X[i+W]    \n",
    "        delta[i] = X[i+W]\n",
    "\n",
    "    if last:\n",
    "        n = len(X)-1\n",
    "        return (feat, labl, delta, np.concatenate((time[n-W:n] - time[n], X[n-W:n] - X[n])), X[n])\n",
    "    return (feat, labl, delta)\n",
    "\n",
    "# get features for all the series with given window size\n",
    "def getFeaturesForAllSeries(all_series, keys, W=7):\n",
    "    all_series_features = [(x[0], getMovingWindowFeatures(x[1], W, True)) for x in all_series]\n",
    "    train_feat, train_labl = [], []\n",
    "    test_feat, test_delta = [], []\n",
    "    for (x,(feat,lbl,_,pf,delta)) in all_series_features:\n",
    "        for (f, l) in zip(feat, lbl):\n",
    "            train_feat.append(dict(zip (keys, list(x)+f.tolist())))\n",
    "            train_labl.append(l)\n",
    "        test_feat.append(dict(zip (keys, list(x)+pf.tolist())))\n",
    "        test_delta.append(delta)        \n",
    "    return ((train_feat, train_labl), (test_feat, test_delta))\n",
    "\n",
    "# get features for all the series with given window size and split into training and testing datasets\n",
    "def getFeaturesForAllSeriesWithSplit(all_series, keys, W=7, train_frac = 0.5):\n",
    "    all_series_features = [(x[0], getMovingWindowFeatures(x[1], W)) for x in all_series]    \n",
    "    train_feat, train_labl = [], []\n",
    "    test_feat, test_labl = [], []\n",
    "    test_delta = []    \n",
    "    for (x,(feat,lbl, delta)) in all_series_features:\n",
    "        t_size = int(len(feat)*train_frac)    \n",
    "        for (f, l) in zip(feat[:t_size], lbl[:t_size]):\n",
    "            train_feat.append(dict(zip (keys, list(x)+f.tolist())))\n",
    "            train_labl.append(l)\n",
    "        for (f, l) in zip(feat[t_size:], lbl[t_size:]):\n",
    "            test_feat.append(dict(zip (keys, list(x)+f.tolist())))\n",
    "            test_labl.append(l)\n",
    "        test_delta = test_delta + delta[t_size:]    \n",
    "    return ((train_feat, train_labl), (test_feat, test_labl, test_delta))\n",
    "\n",
    "# Transform Features into Vectorize form\n",
    "def transformFeatures(train_feat, test_feat, vec = DictVectorizer()):\n",
    "    train_feat_t = vec.fit_transform(train_feat).toarray()\n",
    "    test_feat_t = vec.transform(test_feat).toarray()\n",
    "    return (train_feat_t, test_feat_t)\n",
    "\n",
    "def splitData(all_series_fna, keys, W=7, train_frac = 0.5):\n",
    "    all_series_fna_featured = [(x[0], toWindowFeatures(x[1], 5)) for x in all_series_fna]    \n",
    "    train_feat, train_labl = [], []\n",
    "    test_feat, test_labl = [], []\n",
    "    test_delta = []    \n",
    "    for (x,(feat,lbl, delta)) in all_series_fna_featured:\n",
    "        t_size = int(len(feat)*train_frac)    \n",
    "        for (f, l) in zip(feat[:t_size], lbl[:t_size]):\n",
    "            train_feat.append(dict(zip (keys, list(x)+f.tolist())))\n",
    "            train_labl.append(l)\n",
    "        for (f, l) in zip(feat[t_size:], lbl[t_size:]):\n",
    "            test_feat.append(dict(zip (keys, list(x)+f.tolist())))\n",
    "            test_labl.append(l)\n",
    "        test_delta = test_delta + delta[t_size:]    \n",
    "    return ((train_feat, train_labl), (test_feat, test_labl, test_delta))\n",
    "\n",
    "# Run Improved RFR with given dataframe df, Window Size W and T as in # of predictions to do\n",
    "def runIRFRForDF(df, groupBY, keys, truck_types_to_club,  W=7, T=1):\n",
    "    rfr = RandomForestRegressor(n_estimators=100, \n",
    "                            criterion='mae', max_depth=None, \n",
    "                            min_samples_split=2, min_samples_leaf=1, \n",
    "                            max_features='auto', max_leaf_nodes=None, \n",
    "                            bootstrap=True, oob_score=False, n_jobs=-1, \n",
    "                            random_state=None, verbose=0)\n",
    "    all_series = getSeriesWithoutResample(df, groupBY, truck_types_to_club)\n",
    "    ((train_feat, train_labl), (test_feat, delta)) = getFeaturesForAllSeries(all_series, keys)\n",
    "    # Transform the features\n",
    "    (train_feat_t, test_feat_t) = transformFeatures(train_feat, test_feat)\n",
    "    # Learn IRFR\n",
    "    rfr.fit(train_feat_t, train_labl)\n",
    "    pred = rfr.predict(test_feat_t)    \n",
    "    pred_act = []\n",
    "    for i in range(len(pred)):\n",
    "        pred_act.append(pred[i]+delta[i])\n",
    "    return {(\":\".join([t[k] for k in groupBY])):(p, t) for t,p in zip(test_feat, pred_act)}        \n",
    "\n",
    "def learnRFR(train_feat_t, train_labl):\n",
    "    rfRegress = RandomForestRegressor(n_estimators=100, \n",
    "                            criterion='mae', max_depth=None, \n",
    "                            min_samples_split=2, min_samples_leaf=1, \n",
    "                            max_features='auto', max_leaf_nodes=None, \n",
    "                            bootstrap=True, oob_score=False, n_jobs=-1, \n",
    "                            random_state=None, verbose=0)\n",
    "    rfRegress.fit(train_feat_t, train_labl)\n",
    "    return rfRegress\n",
    "\n",
    "def checkNull(test_feat_t):\n",
    "    return [e for e in test_feat_t if not np.isfinite(e).all()]\n",
    "\n",
    "\n",
    "def IRFR(kpi):\n",
    "    all_series = getSeriesWithResampleAndFNA(kpi)\n",
    "    ((train_feat, train_labl), (test_feat, delta)) = makeFeaturesAndSplit(all_series)\n",
    "    (train_feat_t, test_feat_t) = transformFeatures(train_feat, test_feat)\n",
    "    rfr=learnRFR(train_feat_t, train_labl)\n",
    "    pred = rfr.predict(test_feat_t)    \n",
    "    pred_act = []\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_act.append(pred[i]+delta[i])\n",
    "    return (test_feat, pred_act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
